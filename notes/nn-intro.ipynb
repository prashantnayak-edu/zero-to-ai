{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks\n",
    "\n",
    "Neural Networks are just a specific class of mathematical expressions. The mathematical expression is designed to recognize patterns and relationships in data. \n",
    "\n",
    "They are inspired by the biological neural networks found in animal brains and are loosely based on the currently understood biology of neurons.\n",
    "\n",
    "### Biological Motivation\n",
    "\n",
    "![Biological Neuron](https://cs231n.github.io/assets/nn1/neuron.png)\n",
    "\n",
    "The basic computational unit of the brain is a *neuron*. The brain contains approximately 86 billion neurons, each connected to others via *synapses*. The diagram above is a simplified (cartoon) illustration of a biological neuron.\n",
    "\n",
    "The input structures of a neuron are called *dendrites*, and a neuron can have multiple dendrites that receive signals from other neurons. The output structure is called an *axon*, which can branch out and connect to the dendrites of many other neurons.\n",
    "\n",
    "Our current understanding is that the nervous system delivers inputs to the brain in the form of electrical signals. These signals are received by the dendrites of neurons. If a neuron’s input signals reach a certain threshold, the neuron “fires,” sending an electrical impulse down its axon. This output signal is then transmitted to the dendrites of connected neurons, potentially causing them to fire as well. This chain reaction enables complex communication within the brain, forming the basis of all our thoughts, feelings, and actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Model \n",
    "\n",
    "Neural networks are artificial, mathematical representations of a network of neurons. The goal is to create this network and to *train* it to recognize patterns and relationships in data.\n",
    "\n",
    "To better understand how neurons in neural networks work, let's start with a simple equation that you might already be familiar with from algebra.\n",
    "\n",
    "#### Simple Linear Equation Example\n",
    "\n",
    "In algebra, the equation of a straight line is given by:\n",
    "\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "\n",
    "- y: The output or dependent variable.\n",
    "- x: The input or independent variable.\n",
    "- m: The slope of the line, representing how steep the line is. This can be thought of as a weight that scales the input.\n",
    "- c: The y-intercept, which shifts the line up or down on the graph.\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- The output y depends on the input x multiplied by the weight m, plus a constant c.\n",
    "- Changing the value of m changes how much x influences y. A larger m means x has a bigger impact on y.\n",
    "\n",
    "#### Extending to Multiple Inputs\n",
    "\n",
    "If we have more than one input, the equation extends to:\n",
    "\n",
    "$$\n",
    "y = m_1 x_1 + m_2 x_2 + m_3 x_3 + \\dots + c\n",
    "$$\n",
    "\n",
    "- $x_1, x_2, x_3, \\dots$: Multiple input variables.\n",
    "- $m_1, m_2, m_3, \\dots$: Corresponding weights for each input.\n",
    "\n",
    "Each input x_i is multiplied by its weight m_i, and all the products are added together along with the constant c to produce the output y.\n",
    "\n",
    "#### Connecting to the Neuron model\n",
    "\n",
    "![Mathematical model of a Neuron](https://cs231n.github.io/assets/nn1/neuron_model.jpeg)\n",
    "\n",
    "- Inputs $x_i$: Represented by the arrows entering the neuron.\n",
    "- Weights $w_i$: Associated with each input, indicating the strength of the connection.\n",
    "- Summation $Σ$: The neuron sums all the weighted inputs.\n",
    "- Activation Function $f$: Applied to the weighted sum to produce the output.\n",
    "\n",
    "In the computational model of a neuron, we use a similar idea:\n",
    "\n",
    "1. Inputs and Weights:\n",
    "   - The neuron receives multiple inputs $(x_0, x_1, x_2, \\dots)$, each representing a signal or piece of data.\n",
    "   - Each input has an associated weight $(w_0, w_1, w_2, \\dots)$ that determines its influence on the neuron's output.\n",
    "\n",
    "2. Weighted Sum:\n",
    "   - The neuron calculates a weighted sum of its inputs:\n",
    "     $$\n",
    "     Σ = w_0 x_0 + w_1 x_1 + w_2 x_2 + \\dots + b\n",
    "     $$\n",
    "   - b is the bias term, similar to the constant c in the linear equation, which allows us to shift the activation function to the left or right.\n",
    "\n",
    "3. Activation Function:\n",
    "   - The neuron applies an activation function f to the weighted sum to determine the output:\n",
    "     $$\n",
    "     \\text{Output} = f(Σ)\n",
    "     $$\n",
    "   - The activation function introduces non-linearity, enabling the network to learn complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Weights and Bias:\n",
    "\n",
    "- Weights (w_i):\n",
    "  - Determine how much each input influences the output.\n",
    "  - Can be positive or negative:\n",
    "    - Positive Weight: The input has an excitatory effect, increasing the output.\n",
    "    - Negative Weight: The input has an inhibitory effect, decreasing the output.\n",
    "- Bias (b):\n",
    "  - Allows the neuron to adjust the output independently of the inputs.\n",
    "  - Similar to the c in y = mx + c, shifting the activation function.\n",
    "\n",
    "#### Activation Function:\n",
    "\n",
    "- The activation function f decides whether the neuron should \"fire\" based on the weighted sum.\n",
    "- A common choice is the sigmoid function (σ), which maps any real number into a value between 0 and 1:\n",
    "\n",
    "  σ(z) = 1 / (1 + e^(-z))\n",
    "\n",
    "  where z is the weighted sum.\n",
    "- This function is helpful because:\n",
    "  - It introduces non-linearity, allowing the network to learn more complex patterns.\n",
    "  - The output can be interpreted as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together:\n",
    "\n",
    "1. Step 1: Each input x_i is multiplied by its weight w_i.\n",
    "2. Step 2: All the weighted inputs are summed together along with the bias b.\n",
    "3. Step 3: The activation function f is applied to the sum to produce the neuron's output.\n",
    "4. Step 4: The output can then be sent to other neurons in the network.\n",
    "\n",
    "#### Example with Numbers:\n",
    "\n",
    "Let's consider a simple neuron with two inputs:\n",
    "\n",
    "- Inputs: $x_1 = 2$, $x_2 = 3$\n",
    "- Weights: $w_1 = 0.5$, $w_2 = -1$\n",
    "- Bias: $b = 1$\n",
    "\n",
    "##### Calculate the Weighted Sum:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Weighted Sum} & = w_1 x_1 + w_2 x_2 + b \\\\\n",
    "& = (0.5)(2) + (-1)(3) + 1 \\\\\n",
    "& = 1 - 3 + 1 \\\\\n",
    "& = -1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "##### Apply Activation Function (Sigmoid):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Output} & = \\sigma(-1) \\\\\n",
    "& = \\frac{1}{1 + e^{-(-1)}} \\\\\n",
    "& = \\frac{1}{1 + e^{1}} \\\\\n",
    "& \\approx 0.2689\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "#### Interpretation:\n",
    "\n",
    "- The neuron's output is approximately 0.27.\n",
    "- Since the output is closer to 0, the neuron is less likely to \"fire\" strongly.\n",
    "\n",
    "#### Why This Matters:\n",
    "\n",
    "- By adjusting the weights and bias, the neuron can learn to produce desired outputs for given inputs.\n",
    "- In a network, multiple neurons work together, each learning different patterns.\n",
    "- This simple mathematical model forms the basis for complex neural networks capable of tasks like image recognition, language translation, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways \n",
    "\n",
    "- Neurons in Neural Networks:\n",
    "  - Function similarly to equations you're familiar with, like y = mx + c.\n",
    "  - Use weights to determine the influence of each input.\n",
    "  - Apply activation functions to introduce non-linear behavior.\n",
    "\n",
    "- Learning Process:\n",
    "  - The network learns by adjusting the weights and biases based on data.\n",
    "  - The goal is to minimize the difference between the predicted outputs and the actual outputs.\n",
    "\n",
    "- Real-World Applications:\n",
    "  - Neural networks are used in various technologies you interact with daily, such as smartphone voice assistants, facial recognition systems, and recommendation algorithms.\n",
    "\n",
    "By relating neural networks to concepts you already understand, like linear equations, it's easier to grasp how they work and why they're powerful tools in modern technology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
