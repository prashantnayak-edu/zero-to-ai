{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Vector Embeddings\n",
    "\n",
    "We have learnt what a vector is in the lesson [Data Structures - Scalars, Vectors, Matrices](./notes/data-structs.ipynb) and seen them used to train a basic neural network in [Neural Networks Part 2 - Training (MicroGrad)](./notes/nn-training.ipynb).\n",
    "\n",
    "In this lesson, we will learn about a special type of vector called an **embedding** and how it is used to represent words in a text.\n",
    "\n",
    "### Why Do We Need Numerical Representations of Data?\n",
    "\n",
    "In machine learning and data analysis, algorithms typically work with numerical data. To make predictions or find patterns, we need to represent our data in a way that algorithms can process mathematically.\n",
    "\n",
    "Remember our basic linear regression model?\n",
    "\n",
    "$$\n",
    "y = mx + c\n",
    "$$\n",
    "\n",
    "Here, we need to find the values of $m$ and $c$ that best fit the data. We do this by minimizing the error between the predicted values and the actual values.  Remember also that $x$ can be a scalar or a vector, i.e. have multiple values or features.\n",
    "\n",
    "E.g. if we have a dataset of house prices, we might have a vector of features such as the number of bedrooms, the number of bathrooms, the size of the house, the location, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data for four different houses\n",
    "# [Size (square feet), Number of bedrooms, Number of bathrooms]\n",
    "X = np.array([\n",
    "    [1500, 2, 1],   # house 1\n",
    "    [2500, 3, 2],   # house 2   \n",
    "    [3000, 4, 2],   # house 3\n",
    "    [3500, 5, 3]    # house 4\n",
    "])\n",
    "\n",
    "# House prices (hundereds of thousands of dollars) for the same four houses above\n",
    "y = np.array([200000, 300000, 350000, 400000])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the price of a house with 3 bedrooms, 2 bathrooms, and 1500 square feet\n",
    "new_house = np.array([[1750, 3, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted price of the house is $225,000.00\n"
     ]
    }
   ],
   "source": [
    "predicted_price = model.predict(new_house)\n",
    "print(f\"The predicted price of the house is ${predicted_price[0]:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "We have a dataset of four houses with their features and prices. We want to predict the price of a new house with 3 bedrooms, 2 bathrooms, and 1500 square feet. We use the *LinearRegression* model (this is using a python library called *sklearn*) to find the best fit line between the features and the price. We then use the *predict* method to predict the price of the new house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges with text data\n",
    "\n",
    "Sometimes our data is not numerical like the house prices example above.  If our data is text, then we need to convert this text into a numerical representation.  This is where **embeddings** come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an embedding?\n",
    "\n",
    "An embedding is simply a clever way of turning words into numbers. A Vector embedding is just the same data reprsented as a vector.\n",
    "\n",
    "The term embed means to place something firmly into a space.  So we are placing our text into a vector space - i.e. the space of numbers that can mathematically represent our text. \n",
    "\n",
    "Typically if our input is a set of text (e.g. a chapter from a book), then the first step is to break this text into individual words.  We call this process **tokenization**. \n",
    "\n",
    "Then, each word (or token) is converted into a numberical vector. This numerical vector is called the **vector embedding** of the word. \n",
    "\n",
    "Lets use a simple example.  If we have three words - cat, dog and mouse.  We can represent each of these as a three dimensional vector. \n",
    "\n",
    "For cat we make the first value in the vector 1.  \n",
    "\n",
    "So $cat = [1, 0, 0]$.\n",
    "\n",
    "For dog we make the second value in the vector 1. \n",
    "\n",
    "So $dog = [0, 1, 0]$.\n",
    "\n",
    "For mouse we make the third value in the vector 1. \n",
    "\n",
    "So $mouse = [0, 0, 1]$.\n",
    "\n",
    "This approach of using a different value with 1 is called a **one-hot encoding**. This is because only one of the values in a vector is 1 for any particular word and the rest are 0. \n",
    "\n",
    "Of course you can see the limitation here, in that there is only three words we can represent with this approach. The minute we add another word, we have to increase the size of our vectors by 1.\n",
    "\n",
    "Lets next look at a slightly more sophisticated approach. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
