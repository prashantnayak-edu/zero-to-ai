# Zero to AI


The lessons in this course are around Neural Networks and the Transformer architecture that is at the root of the current AI revolution.

I put this course together to both learn about and potentially educate others that are on the journey to understand the current state of AI.

My goal was to try and explain it to myself in a way that a motivated High School student could follow along.

The material here is sourced from:
 - [Andrej Karpathy - NN-Zero-To-Hero](https://github.com/karpathy/nn-zero-to-hero)
 - [Vik Parchuri - Zero_To_GPT](https://github.com/VikParuchuri/zero_to_gpt/tree/master?tab=readme-ov-file)
 - [Tivadar Danka - Mathematics of Machine Learining](https://tivadardanka.com/mathematics-of-machine-learning-preview)
 - [CS-231 Stanford University](https://cs231n.github.io/)

1. Math Fundamentals
Lessons on basics of linear algerba and calculus
  - [Derivative of a single variable](/notes/derivative-single-var.ipynb)
  - [Vectors](/notes/vector.ipynb)
  - [Gradient Descent](/notes/gradient-descent.ipynb)
  - [Stochastic Gradient Descent](/notes/sgd.ipynb)

2. Introduction to Neural Networks
  - [Introduction to Neural Networks](/notes/nn-intro.ipynb)
  - [Types of Neural Networks](/notes/types-of-neural-networks.ipynb)
  - [Learning from data](/notes/learning-from-data.ipynb)
  - [MicroGrad: Spelled out intro to neural networks](/notes/micrograd1.ipynb)
  - [MicroGrad: Training a Neural Network (Backpropagation)](/notes/micrograd2.ipynb)

3. Dense Neural Networks

4. Classification with Neural Networks

5. Recurrent Neural Networks

6. Optimization and Regularization

7. Vector Embeddings

8. Working with Text (basicas of GPT)

9. Transformers

10. Building GPT-2




